{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview and Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Team Members: Vamsi Banda, Rhett D’souza , Lukas Justen and Keith Pallo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project our team set out to train a deep neural network by leveraging a PyTorch stack on top of AWS. Our goals were to gain experience in wrangling a large dataset and using some of the newer technologies that have gained traction in industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular project our overarching goal was to perform image classification on the Cifar 10 dataset. This is a classic research dataset consisting over 30,000 images with 10 discrete output classes. Some example images from the dataset can be seen here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Example of Cifar 10 Data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cifar_sample](documentation_images/cifar_examples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project consists of several different Jupyter Notebooks and Python scripts which are each run in different AWS services in the cloud. This notebook serves as the primary documentation for an overview of the completed work - but all referenced files can be found in the associated directories as described below\n",
    "\n",
    "Link to Cifar 10 Website: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS has many different options for training, validating, and deploying deep learning models - so at the start of the project it was a somewhat daughting task to choose a pacticular architecture. \n",
    "\n",
    "However, after looking through the available options our team decided to utilize the newer Sagemaker service - which is what AWS calls a \"managed service\". Sagemaker allows developers to quickly bundle together different services that AWS offers (like classic compute which is called EC2) and elastic storage (called S3) very easily, using custom built commands. All of the code can be run from a \"SageMaker\" Notebook instance, which can contain multiple different file types, including Jupyter Notebooks that can import the custom SageMaker code. Additionally, because this is a managed service, a significant amount of setup, like making sure the correct packages have been installed has already been handled. This is a huge advantage of the system, as we do not have to be concerned with configuration issues on the remote systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our particular application we have chosen to train Cifar10 by uploading the raw data to our own S3 bucket, using EC2 to train the model, and then storing the model hyperparamters in the same S3 bucket. Then, we also configured deployment of our model using AWS services Lambda and API Gateway so we can test our trained models. We also created a simple android application using the new Google language Flutter - which is included in our submission for reference as well. Below is a general overview of the architecture, but we also go further in depth concerning deployment in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"documentation_images/sagemaker-architecture.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our task we created several different aws source files which have included for reference. Below is a list of their name and purposes. \n",
    "\n",
    "### Notable Files\n",
    "     \n",
    "     ├── documentation_images     (directory)      # Holds reference images\n",
    "     \n",
    "     ├── aws_full_reference_files (directory)\n",
    "          ├── DeepLearningModel.ipynb              # Contains code to setup connections \n",
    "          ├── pytorch_cifar.py                     # Core model code\n",
    "          \n",
    "     ├── android_app (directory)\n",
    "          ├── app-release.apk                      # Android app (created from Flutter)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our environment - Sagemaker Connections and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is executed in a SageMaker Jupyter Notebook Instance to setup our S3 buckets, prepare the data for training and testing. This is a great example of the available SageMaker commands available - where we can directly interface with other AWS services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Import sagemaker packages and define assocaited roles for this particular instance \n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket= 'dlbucket435'\n",
    "data_key = 'cifar-10-python.tar.gz'\n",
    "model_out = 'model_1'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "download_in = './cifar'\n",
    "\n",
    "# Data Preperation for data viewing, testing and usage in the main script (pytorch_cifar.py).\n",
    "\n",
    "# Transformation functions\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Setup output classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Loading example for a CIFAR10 local Dataset and/or fresh dataset download with transformation\n",
    "trainset = torchvision.datasets.CIFAR10(root=download_in, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Unzip local version of CIFAR-10 dataset(we can use the AWS version of cifar10)\n",
    "!tar -zcvf cifar_10.tar.gz ./cifar\n",
    "\n",
    "# Raw Data Uploaded to S3 to be downloaded by the estimator's fit function later on during training with inputs URI\n",
    "inputs = sess.upload_data(path='cifar', bucket=bucket)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs)) \n",
    "\n",
    "\n",
    "              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Model - Modified Lenet and VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our PyTorch model, in the SageMaker environment, we must create a SageMaker Pytorch estimator that calls an entry point script. The script must implement key methods required by the SageMaker estimator interface noted in the [SageMaker documentation](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/pytorch).\n",
    "```python\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "# Define the estimator\n",
    "estimator = PyTorch(entry_point='pytorch_cifar.py',\n",
    "                            role=role,\n",
    "                            framework_version='1.0.0',\n",
    "                            train_instance_count=1,\n",
    "                            train_instance_type='ml.c4.8xlarge',\n",
    "                            output_path='s3://dlbucket435/model/'\n",
    "                            )\n",
    "\n",
    "# Fit the model with the training data\n",
    "estimator.fit({'training':inputs})\n",
    "```\n",
    "The fit function initiates the training job, spins up the AWS Compute Instance ( which in our case is a `ml.c4.8xlarge` instance), feeds the data from the S3 bucket to the training job, and starts training.\n",
    "The model generated and any logs/checkpoints that we wished to write to the local instance and hence save (to local directory `model_dir`, in the entry point script) get written to the path specified by `output_path` in the PyTorch estimator object instantiation. In this case, we send it out to the S3 bucket __dlbucket435__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the entry point script, we define our PyTorch models to be trained, the cost function, optimizers, data I/O, the training procedure, model saving, model loading and prediction.\n",
    "\n",
    "We trained 2 networks, a modified version of LeNet and VGG11. We used the Adam optimizer with a Cross Entropy Loss function. For reference, the model that is currently deployed in our endpoint (and hence used by our application) is VGG11.\n",
    "\n",
    "Class for the wider version (32 channels and 64 channels) of LeNet:\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "Class for VGG architectures, with VGG11 being selected and trained:\n",
    "```python\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the training data (similar to `trainloader` as mentioned earlier) from the temporary local directory (after it is downloaded from S3), normalize the data and then begin training.\n",
    "Refer to the entry point script (pytorch_cifar.py) to view line-by-line definition of the training process.\n",
    "\n",
    "After the model has been successfully trained, we load the SageMaker PyTorchModel, and deploy the model onto an endpoint to be used for inference.\n",
    "\n",
    "```python\n",
    "pytorch_model = PyTorchModel(\n",
    "model_data='s3://dlbucket435/model/sagemaker-pytorch-2019-03-18-09-59-04-255/output/model.tar.gz', role=role,\n",
    "                             entry_point='pytorch_cifar.py')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.c4.8xlarge', initial_instance_count=1)\n",
    "```\n",
    "\n",
    "The `deploy` function spins up the Compute Instance `ml.c4.8xlarge` and deploys the model using the function `model_fn` implemented in the entry point script to load the model from the S3 bucket (stored in the `output_path`)\n",
    "\n",
    "This endpoint is then used in the next section, to serve the PyTorch model for live inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment - Lambda and API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AppGatewayLambdaSagemaker.png \"Architecture for the deployment part of our project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobile Application\n",
    "\n",
    "We used the mobile app development framework Flutter in order to build a small mobile app that can take a picture, sample it down to a 32 x 32 image which will then be classified by our model. To invoke the infer function of the model we needed to build something that allows our app to connect to AWS. Luckily, AWS provides its users with an API Gateway. After the app invokes that API and receives a classfication of the image the app displays the classification. The android version of our application has been provided if testing is desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Gateway\n",
    "\n",
    "As already mentioned, the API Gateway allows our app to connect to the model which we have created using Amazon Sagemaker. The API Gateway is a small web interface that allows you to build a simple REST API. We can then use that Gateway to AWS to connect to a Lambda function which will handle the user's request to classify the image. We are using a simple POST method to upload an base64 encoded image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Function\n",
    "\n",
    "AWS provides users with Lambda functions that can run your code in response to events. In our case this event will be a request that has been received by the API Gateway. The Lambda function atomatically manages the compute resources for the users which makes it very convenient and scaleable to use these Lambda functions for inference. The Lambda function consists of one python script with a single function that is called in reponse to the received event. In our case, the python script will then convert the image into the proper representation for the trained network. After Sagemaker returns a classification the Lambda function return the response to the API Gateway and finally to the mobile application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    data = json.loads(json.dumps(event))\n",
    "    payload = data['data']\n",
    "    bytes = str.encode(payload)\n",
    "    png = base64.decodebytes(bytes)\n",
    "    q = np.frombuffer(png, dtype=np.uint8)\n",
    "    q = q.reshape((1, 3, 32, 32))\n",
    "    data = q.tolist().__str__()\n",
    "    \n",
    "    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                      ContentType='application/json',\n",
    "                                      Body=data)\n",
    "                                      \n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Endpoint\n",
    "\n",
    "The model that has been created by the Sagemaker notebook and the Pytorch Python script can be stored in an S3 bucket on our AWS machine. Instead of saving one model we could also store different versions of our model. In the case of a bug in our model, we could easily switch the model that is used by the Lambda function for inference. All in all, a group source of capability for our system and AWS in general is that the mentioned components can be pluged into each other like Legos. This makes it very convenient to build a deep learning pipeline using Amazon Sagemaker and AWS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Application Output\n",
    "\n",
    "<img src=\"documentation_images/SampleOutput.png\" height=\"700\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this project, we had several key learnings around the implementation of large deep learning infrastructure. \n",
    "\n",
    "Firstly, we experienced a steep learning curve for getting things going on the AWS cloud platform. Although AWS has done a good job of documenting specific use cases, it can be hard to figure out where to start. For example, there are several different ways to collaborate across multiple users (AWS Organizations, IAM Users, etc.) but in order to determine the best method, we had to reach out to an experienced AWS developer in our network. Additionally, debugging a system utilizing a managed service can be quite difficult, because a significant amount of configuation has been abstracted away. This difficulty extends to potentially buggy deployments - where there can be a 5 minute lead time between sending a new deployment and testing if it is operational.\n",
    "\n",
    "However, despite these difficulties, our group very much enjoyed learning about AWS and the benefits are massive. For example, when deploying our model to an application it was extremely easy to do so - a task that would have been daughting if we had not used the platform. Additionally, the ability to \"plug and play\" with our models was astounding - and after solving some initial system OS issues, it became clear how anyone (ranging from an individual, a startup, or a Fortune 50 company) could very quickly get going with at scale deep learning. \n",
    "\n",
    "Furthermore, we would like to thank Professor Aggelos Katsaggelos for an amazing Deep Learning course experience! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
